[2022-05-13 11:13:58,080] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_aluraonline 2022-05-12T09:00:00+00:00 [queued]>
[2022-05-13 11:13:58,091] {taskinstance.py:670} INFO - Dependencies all met for <TaskInstance: twitter_dag.transform_twitter_aluraonline 2022-05-12T09:00:00+00:00 [queued]>
[2022-05-13 11:13:58,091] {taskinstance.py:880} INFO - 
--------------------------------------------------------------------------------
[2022-05-13 11:13:58,091] {taskinstance.py:881} INFO - Starting attempt 2 of 2
[2022-05-13 11:13:58,091] {taskinstance.py:882} INFO - 
--------------------------------------------------------------------------------
[2022-05-13 11:13:58,097] {taskinstance.py:901} INFO - Executing <Task(SparkSubmitOperator): transform_twitter_aluraonline> on 2022-05-12T09:00:00+00:00
[2022-05-13 11:13:58,099] {standard_task_runner.py:54} INFO - Started process 26629 to run task
[2022-05-13 11:13:58,148] {standard_task_runner.py:77} INFO - Running: ['airflow', 'run', 'twitter_dag', 'transform_twitter_aluraonline', '2022-05-12T09:00:00+00:00', '--job_id', '45', '--pool', 'default_pool', '--raw', '-sd', 'DAGS_FOLDER/twitter_dag.py', '--cfg_path', '/tmp/tmpf_7voui0']
[2022-05-13 11:13:58,149] {standard_task_runner.py:78} INFO - Job 45: Subtask transform_twitter_aluraonline
[2022-05-13 11:13:58,169] {logging_mixin.py:112} INFO - Running <TaskInstance: twitter_dag.transform_twitter_aluraonline 2022-05-12T09:00:00+00:00 [running]> on host LAV3K867Z2L.interno.inflexion.com.br
[2022-05-13 11:13:58,206] {base_hook.py:89} INFO - Using connection to: id: spark_default. Host: local, Port: None, Schema: None, Login: None, Password: None, extra: XXXXXXXX
[2022-05-13 11:13:58,208] {spark_submit_hook.py:325} INFO - Spark-Submit cmd: spark-submit --master local --name twitter_transformation /home/william/Documentos/ESTUDOS/ALURA-ENGENHARIA_DADOS/datapipeline/spark/transformation.py --src /home/william/Documentos/ESTUDOS/ALURA-ENGENHARIA_DADOS/datapipeline/datalake/bronze/twitter_aluraonline/extract_date=2022-05-12 --dest /home/william/Documentos/ESTUDOS/ALURA-ENGENHARIA_DADOS/datapipeline/datalake/silver/twitter_aluraonline/ --process-date 2022-05-12
[2022-05-13 11:13:58,211] {taskinstance.py:1150} ERROR - [Errno 2] No such file or directory: 'spark-submit'
Traceback (most recent call last):
  File "/home/william/Documentos/ESTUDOS/ALURA-ENGENHARIA_DADOS/datapipeline/.env/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 984, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/home/william/Documentos/ESTUDOS/ALURA-ENGENHARIA_DADOS/datapipeline/.env/lib/python3.8/site-packages/airflow/contrib/operators/spark_submit_operator.py", line 187, in execute
    self._hook.submit(self._application)
  File "/home/william/Documentos/ESTUDOS/ALURA-ENGENHARIA_DADOS/datapipeline/.env/lib/python3.8/site-packages/airflow/contrib/hooks/spark_submit_hook.py", line 390, in submit
    self._submit_sp = subprocess.Popen(spark_submit_cmd,
  File "/usr/lib/python3.8/subprocess.py", line 858, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File "/usr/lib/python3.8/subprocess.py", line 1704, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
FileNotFoundError: [Errno 2] No such file or directory: 'spark-submit'
[2022-05-13 11:13:58,213] {taskinstance.py:1187} INFO - Marking task as FAILED. dag_id=twitter_dag, task_id=transform_twitter_aluraonline, execution_date=20220512T090000, start_date=20220513T141358, end_date=20220513T141358
[2022-05-13 11:14:03,086] {local_task_job.py:102} INFO - Task exited with return code 1
